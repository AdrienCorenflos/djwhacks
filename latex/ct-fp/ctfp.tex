\documentclass[mathserif]{beamer}
%\documentclass{beamer}
\usetheme{Luebeck}
\usecolortheme{spruce}
\usecolortheme{rose}
\usepackage{amsmath,verbatim}
\usepackage{listings}
\usepackage[english]{babel}
%\usepackage{media9} % package for genuine embedding of movies
%\usepackage{multimedia} % beamer package for calling external files and players
\setbeamercovered{transparent}

% media9 stuff for playing movies
%\newcommand{\includemovie}[3]{\includemedia[width=#1,height=#2,activate=pagevisible,deactivate=pageclose,addresource=#3,flashvars={src=#3 &autoPlay=true &loop=true &controlBarAutoHideTimeout=0 }]{}{VPlayer.swf}}

\newcommand{\trans}{\ensuremath{{}^\mathrm{T}}}
\newcommand{\eps}{\varepsilon}
\newcommand*{\approxdist}{\mathrel{\vcenter{\offinterlineskip
\vskip-.25ex\hbox{\hskip.55ex$\cdot$}\vskip-.25ex\hbox{$\sim$}
\vskip-.5ex\hbox{\hskip.55ex$\cdot$}}}}

\lstdefinelanguage{myR}
{
   language=R,
   otherkeywords={read.table, set.seed, head},
   deletekeywords={url,codes, t, dt, Call, formula,Q, R, on,by,hat,is,
col, set,start,end,deltat,zip},
   sensitive=true,
   breaklines=true,
   morecomment=[l]{\#},
   morestring=[b]",
   morestring=[b]',
   basicstyle =\ttfamily\small,
   keywordstyle=\bfseries,
   showtabs=false,
   showstringspaces=false,
   literate= {~}{$\sim$}{2},
   numberstyle=\sffamily\scriptsize,
   stepnumber=2
 }

\lstset{basicstyle=\ttfamily\color{blue}}

\begin{document}

\title[CT and FP for scalable statistical computing]{An introduction to category theory and functional programming for scalable statistical modelling and computation}
\author[Darren Wilkinson --- Statistics seminar, 3/2/17]{\textbf{\large Darren Wilkinson} \\
\url{@darrenjw}\\
\alert{\url{tinyurl.com/darrenjw}}\\
School of Mathematics \& Statistics\\Newcastle University, UK}
\date{Statistics Seminar\\Newcastle University\\3rd February 2017}

\frame{\titlepage}

\section{Introduction}

\subsection{Preamble}

\frame{
  \frametitle{Preamble}
  \begin{itemize}
  \item This talk isn't yet quite ready for ``prime time''
    \begin{itemize}
    \item Very much ``work in progress''
      \end{itemize}
  \item It's a bit of a ``brain dump''...
    \begin{itemize}
    \item But something I've been thinking about on-and-off for the last 20 years...
    \end{itemize}
  \item Almost at the stage where I can begin to articulate my ideas
    \begin{itemize}
    \item but not quite...
    \end{itemize}
    \end{itemize}
  }

\subsection{Outline}

\frame{
\frametitle{Talk outline}
\begin{itemize}
\item What's wrong with the current state of statistical modelling and computation?
\item What is functional programming (FP) and why is it better than conventional imperative programming?
\item What is category theory (CT) and what has it got to do with FP?
\item How can we use CT and FP to make statistical computing more scalable?
  \item What does ``scalable'' mean, anyway?
\item Some examples...
\end{itemize}
}

\subsection{What's the problem?}

\frame{
  \frametitle{What's up with statistical computing?}
  \begin{itemize}
    \pause
  \item Everything!
    \pause
  \item \alert{R} has become the defacto standard programming language for statistical computing --- it was designed by statisticians for statisticians, and it shows!
    \begin{itemize}
    \item Many dubious language design choices, meaning it will always be ugly, slow and inefficient (without many significant breaking changes to the language)
    \item R's inherent inefficiencies mean that much of the R codebase isn't in R at all, but instead in other languages, such as \alert{Fortran}, \alert{C} and \alert{C++}
      \item Although faster and more efficient that R, these languages are actually all even \alert{worse} languages for statistical computing than R!
      \end{itemize}
  \end{itemize}
  }

\frame{
  \frametitle{Pre-historic programming languages}
  \begin{itemize}
  \item The fundamental problem is that all of the programming languages commonly used for scientific and statistical computing were designed 30-50 years ago, in the dawn of the computing age, and haven't significantly changed
    \begin{itemize}
  \item Think how much computing \alert{hardware} has changed in the last 40 years!
  \item But the language you are using was designed for that hardware using the knowledge of programming languages that existed at that time
  \item Think about how much statistical methodology has changed in the last 40 years --- you wouldn't use 40 year old methodology --- why use 40 year old languages to implement it?!
    \end{itemize}
    \end{itemize}
}
    
\frame{
  \frametitle{Modern programming language design}
  \begin{itemize}
  \item We have learned just as much about programming and programming languages in the last 40 years as we have about everything else
  \item Our understanding has developed in parallel with developments in hardware
  \item People have been thinking a lot about how languages can and should exploit modern computing hardware such as multi-core processors and parallel computing clusters
    \item Modern functional programming languages are emerging as better suited to modern hardware
    \end{itemize}
  }

\section{FP and CT}

\subsection{FP}

\frame{
  \frametitle{What is FP?}
  \begin{itemize}
  \item FP languages emphasise the use of \alert{immutable} data, \alert{pure}, \alert{referentially transparent functions}, and \alert{higher-order functions}
  \item Unlike commonly used \alert{imperative} programming languages, they are closer to the Church end of the \alert{Church-Turing thesis} --- eg. closer to \alert{Lambda--calculus} than a \alert{Turing--machine}
  \item The original Lambda--calculus was \alert{untyped}, corresponding to a \alert{dynamically--typed} programming language, such as \alert{Lisp}
    \item \alert{Statically--typed} FP languages (such as \alert{Haskell}) are arguably more scalable, corresponding to the \alert{simply--typed Lambda--calculus}, closely related to \alert{cartesian closed categories}...
  \end{itemize}
  }

\frame{
  \frametitle{FP}
  \begin{itemize}
  \item bar
  \end{itemize}
  }

\frame{
  \frametitle{Concurrency, programming and shared mutable state}
  \begin{itemize}
  \item bar
  \end{itemize}
  }

\frame{
  \frametitle{Ideal languages for statistical computing}
  \begin{itemize}
  \item bar
  \end{itemize}
  }

\frame{
  \frametitle{Monadic collections}
  \begin{itemize}
  \item bar
  \end{itemize}
  }



\subsection{CT}

\frame{
  \frametitle{foo}
  \begin{itemize}
  \item bar
  \end{itemize}
  }

\frame{
  \frametitle{foo}
  \begin{itemize}
  \item bar
  \end{itemize}
}

\subsection{Scalable statistical computing}

\frame{
  \frametitle{foo}
  \begin{itemize}
  \item bar
  \end{itemize}
  }

\frame{
  \frametitle{foo}
  \begin{itemize}
  \item bar
  \end{itemize}
  }

\section{Summary and conclusions}

\frame{
  \frametitle{foo}
  \begin{itemize}
  \item bar
  \end{itemize}
  }

\frame{
  \frametitle{Conclusions}
  \begin{itemize}
  \item We should approach the problem of statistical modelling and computation in a modular, composable, functional way, guided by underpinning principles from category theory
  \item To implement solutions to problems in statistical modelling and compution in a more scalable way, we need programming languages which are:
    \begin{itemize}
    \item \alert{Strongly statically typed} (but with type inference)
    \item \alert{Compiled} (but possibly to a VM)
    \item \alert{Functional} (with support for immutable values and higher-order functions)
      \item and have support for \alert{typeclasses} and \alert{higher-kinded types}, allowing the adoption of design patterns from category theory
    \end{itemize}
    \item \alert{Scala} and \alert{Spark} provide a nice illustration of the power of this approach, but there are other interesting languages, including: Haskell, ML, OCaml, Frege, Eta, ...
  \end{itemize}
  }

\end{document}

