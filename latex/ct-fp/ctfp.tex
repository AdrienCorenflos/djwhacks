\documentclass[mathserif,handout]{beamer}
%\documentclass{beamer}
\usetheme{Luebeck}
\usecolortheme{spruce}
\usecolortheme{rose}
\usepackage{amsmath,verbatim}
\usepackage{listings}
\usepackage[english]{babel}
%\usepackage{media9} % package for genuine embedding of movies
%\usepackage{multimedia} % beamer package for calling external files and players
\setbeamercovered{transparent}

% media9 stuff for playing movies
%\newcommand{\includemovie}[3]{\includemedia[width=#1,height=#2,activate=pagevisible,deactivate=pageclose,addresource=#3,flashvars={src=#3 &autoPlay=true &loop=true &controlBarAutoHideTimeout=0 }]{}{VPlayer.swf}}

\newcommand{\trans}{\ensuremath{{}^\mathrm{T}}}
\newcommand{\eps}{\varepsilon}
\newcommand*{\approxdist}{\mathrel{\vcenter{\offinterlineskip
\vskip-.25ex\hbox{\hskip.55ex$\cdot$}\vskip-.25ex\hbox{$\sim$}
\vskip-.5ex\hbox{\hskip.55ex$\cdot$}}}}

\lstdefinelanguage{myR}
{
   language=R,
   otherkeywords={read.table, set.seed, head},
   deletekeywords={url,codes, t, dt, Call, formula,Q, R, on,by,hat,is,
col, set,start,end,deltat,zip},
   sensitive=true,
   breaklines=true,
   morecomment=[l]{\#},
   morestring=[b]",
   morestring=[b]',
   basicstyle =\ttfamily\small,
   keywordstyle=\bfseries,
   showtabs=false,
   showstringspaces=false,
   literate= {~}{$\sim$}{2},
   numberstyle=\sffamily\scriptsize,
   stepnumber=2
 }

\lstset{basicstyle=\ttfamily\color{blue}}

% \lstset{language=scala, basicstyle=\ttfamily\small, breaklines=true}

\lstMakeShortInline[language=scala]|


\begin{document}

\title[CT and FP for scalable statistical computing]{An introduction to category theory and functional programming for scalable statistical modelling and computation}
\author[Darren Wilkinson --- Statistics seminar, 3/2/17]{\textbf{\large Darren Wilkinson} \\
\url{@darrenjw}\\
\alert{\url{tinyurl.com/darrenjw}}\\
School of Mathematics \& Statistics\\Newcastle University, UK}
\date{Statistics Seminar\\Newcastle University\\3rd February 2017}


\frame{\titlepage}

\section{Introduction}

\subsection{Outline}

\frame{
\frametitle{Talk outline}
\begin{itemize}
\item What's wrong with the current state of statistical modelling and computation?
\item What is functional programming (FP) and why is it better than conventional imperative programming?
\item What is category theory (CT) and what has it got to do with FP?
\item How can we use CT and FP to make statistical computing more scalable?
  \item What does ``scalable'' mean, anyway?
\item Some examples...
\end{itemize}
}

\subsection{What's the problem?}

\frame{
  \frametitle{What's up with statistical computing?}
  \begin{itemize}
    \pause
  \item Everything!
    \pause
  \item \alert{R} has become the defacto standard programming language for statistical computing --- it was designed by statisticians for statisticians, and it shows!
    \begin{itemize}
    \item Many dubious language design choices, meaning it will always be ugly, slow and inefficient (without many significant breaking changes to the language)
    \item R's inherent inefficiencies mean that much of the R codebase isn't in R at all, but instead in other languages, such as \alert{Fortran}, \alert{C} and \alert{C++}
      \item Although faster and more efficient that R, these languages are actually all even \alert{worse} languages for statistical computing than R!
      \end{itemize}
  \end{itemize}
  }

\frame{
  \frametitle{Pre-historic programming languages}
  \begin{itemize}
  \item The fundamental problem is that all of the programming languages commonly used for scientific and statistical computing were designed 30-50 years ago, in the dawn of the computing age, and haven't significantly changed
    \begin{itemize}
  \item Think how much computing \alert{hardware} has changed in the last 40 years!
  \item But the language you are using was designed for that hardware using the knowledge of programming languages that existed at that time
  \item Think about how much statistical methodology has changed in the last 40 years --- you wouldn't use 40 year old methodology --- why use 40 year old languages to implement it?!
    \end{itemize}
    \end{itemize}
}
    
\frame{
  \frametitle{Modern programming language design}
  \begin{itemize}
  \item We have learned just as much about programming and programming languages in the last 40 years as we have about everything else
  \item Our understanding has developed in parallel with developments in hardware
  \item People have been thinking a lot about how languages can and should exploit modern computing hardware such as multi-core processors and parallel computing clusters
    \item Modern functional programming languages are emerging as better suited to modern hardware
    \end{itemize}
  }

\section{FP and CT}

\subsection{Functional Programming}

\frame{
  \frametitle{What is FP?}
  \begin{itemize}
  \item FP languages emphasise the use of \alert{immutable} data, \alert{pure}, \alert{referentially transparent functions}, and \alert{higher-order functions}
  \item Unlike commonly used \alert{imperative} programming languages, they are closer to the Church end of the \alert{Church-Turing thesis} --- eg. closer to \alert{Lambda--calculus} than a \alert{Turing--machine}
  \item The original Lambda--calculus was \alert{untyped}, corresponding to a \alert{dynamically--typed} programming language, such as \alert{Lisp}
    \item \alert{Statically--typed} FP languages (such as \alert{Haskell}) are arguably more scalable, corresponding to the \alert{simply--typed Lambda--calculus}, closely related to \alert{cartesian closed categories}...
  \end{itemize}
  }

\frame{
  \frametitle{FP}
  \begin{itemize}
  \item In pure FP, all state is \alert{immutable} --- you can assign names to things, but you can't change what the name points to --- no ``variables'' in the usual sense
  \item Functions are \alert{pure} and \alert{referentially transparent} --- they can't have side-effects --- they are just like functions in mathematics...
  \item Functions can be recursive, and \alert{recursion} can be used to iterate over recursive data structures --- useful since no conventional ``for'' or ``while'' loops in pure FP languages
  \item Functions are first class objects, and \alert{higher-order functions} (HOFs) are used extensively --- functions which return a function or accept a function as argument
  \end{itemize}
  }

\begin{frame}[fragile]
  \frametitle{Concurrency, parallel programming and shared mutable state}
  \begin{itemize}
  \item Modern computer architectures have processors with several cores, and possibly several processors
  \item Parallel programming is required to properly exploit this hardware
  \item The main difficulties in parallel and concurrent programming in imperative programming all relate to issues associated with \alert{shared mutable state}
  \item In pure FP, state is not mutable, so there is no mutable state, and hence no shared mutable state
    \item Most of the difficulties associated with parallel and concurrent programming just don't exist in FP --- this has been one of the main reasons for the recent resurgence of FP languages
  \end{itemize}
\end{frame}

\frame{
  \frametitle{Ideal languages for statistical computing}
  \begin{itemize}
  \item We should approach the problem of statistical modelling and efficient computation in a modular, composable, functional way
  \item To do this we need programming languages which are:
    \begin{itemize}
    \item \alert{Strongly statically typed} (but with type inference)
    \item \alert{Compiled} (but possibly to a VM)
    \item \alert{Functional} (with support for immutable values and higher-order functions)
      \item and have support for \alert{typeclasses} and \alert{higher-kinded types}, allowing the adoption of design patterns from \alert{category theory}
    \end{itemize}
  \item For efficient statistical computing, it can be argued that evaluation should be \alert{strict} rather than \alert{lazy} by default
    \item \alert{Scala} is a popular language which meets the above constraints
  \end{itemize}
  }

\begin{frame}[fragile]
  \frametitle{Monadic collections}
  \begin{itemize}
  \item A collection of type |M[T]| can contain (multiple) values of type |T|
  \item If the collection supports a higher-order function\\
    |map(f: T => S): M[S]| then we call the collection a \alert{Functor}
    \begin{itemize}
    \item eg. |List(1,3,5,7) map (x => x*2) = List(2,6,10,14)|
    \end{itemize}
  \item If the collection additionally supports a higher-order function\\
    |flatMap(f: T => M[S]): M[S]| then we call the collection a \alert{Monad}
    \begin{itemize}
    \item eg. |List(1,3,5,7) flatMap (x => List(x,x+1))|\\
      \hspace{6ex} |= List(1, 2, 3, 4, 5, 6, 7, 8)|
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Other monadic types: Option}
  \begin{itemize}
  \item Some computations can fail, and we can capture that possibility with a type called |Option|
    \begin{itemize}
    \item in Scala --- it is |Optional| in Java 8 and |Maybe| in Haskell
      \end{itemize}
    \item An |Option[T]| can contain |Some[T]| or |None|
    \item So if we have |chol: Matrix => Option[TriMatrix]| we can check to see if we have a result
    \item But if we also have |triSolve: (TriMatrix,Vector) => Option[Vector]|, how do we ``compose'' these?
      \begin{itemize}
      \item |chol(mat) map (tm => triSolve(tm,vec))| has type |Option[Option[Vector]]| which isn't quite what we want
      \item |chol(mat) flatMap (tm => triSolve(tm,vec))| has type |Option[Vector]| which we do want
        \item |flatMap| allows \alert{composition} of monadic functions
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Composing monadic functions}
  \begin{itemize}
  \item Given functions |f: S => T|, |g: T => U|, |h: U => V|, we can compose them as |h compose g compose f| or |s => h(g(f(s)))| to get |hgf: S => V|
  \item Monadic functions |f: S => M[T]|, |g: T => M[U]|, |h: U => M[V]| don't compose directly, but do using |flatMap|:\\
    |s => f(s) flatMap g flatMap h| has type |S => M[V]|
  \item Can be written as a ``for-comprehension'' (``do'' in Haskell):\\
    |s => for (t<-f(s); u<-g(t); v<-h(u)) yield v|
    \item Just syntactic sugar for the chained |flatMap|s above --- really \alert{not} an imperative-style ``for loop'' at all...
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Other monadic types: Future}
  \begin{itemize}
  \item A |Future[T]| is used to dispatch a (long-running) computation to another thread to run in parallel with the main thread
  \item When a |Future| is created, the call returns immediately, and the main thread continues, allowing the |Future| to be ``used'' before its result (of type |T|) is computed
  \item |map| can be used to transform the result of a |Future|, and |flatMap| can be used to chain together |Futures| by allowing the output of one |Future| to be used as the input to another
  \item |Future|s can be transformed using |map| and |flatMap| irrespective of whether or not the |Future| computation has yet completed and actually contains a value
    \item |Future|s are a powerful method for developing parallel and concurrent programs in a modular, composable way
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Other monadic types: Prob/Rand}
  \begin{itemize}
  \item The \alert{Probability monad} is another important monad with obvious relevance to statistical computing
  \item A |Rand[T]| represents a random quantity of type |T|
  \item It is used to encapsulate the non-determinism of functions returning random quantities --- otherwise these would break the \alert{purity} and \alert{referential transparancy} of the function
  \item |map| is used to transform one random quantity into another
  \item |flatMap| is used to chain together stochastic functions to create joint and/or marginal random variables, or to \alert{propagate uncertainty} through a computational workflow or pipeline
    \item Probability monads form the basis for the development of \alert{probabilistic programming languages} using FP
    \item The probability monad is typically implemented as a \alert{State monad}, the mechanism for handling mutable state using FP
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Parallel monadic collections}
  \begin{itemize}
  \item Using |map| to apply a \alert{pure} function to all of the elements in a collection can clearly be done in parallel
  \item So if the collection contains $n$ elements, then the computation time can be reduced from $O(n)$ to $O(1)$ (on infinite parallel hardware)
    \begin{itemize}
    \item |Vector(3,5,7) map (_*2) = Vector(6,10,14)|
    \item |Vector(3,5,7).par map (_*2) = ParVector(6,10,14)|
    \end{itemize}
    \item We can carry out \alert{reductions} as \alert{folds} over collections:\\
      |Vector(6,10,14).par reduce (_+_) = 30|
      \item In general, sequential folds can not be parallelised, but...
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Monoids and parallel ``map--reduce''}
  \begin{itemize}
  \item A \alert{monoid} is a very important concept in FP
  \item For now we will think of a monoid as a \alert{set} of elements with a \alert{binary relation} $\star$ which is \alert{closed} and \alert{associative}, and having an \alert{identity} element wrt the binary relation
  \item You can think of it as a \alert{semi-group} with an identity or a \alert{group} without an inverse
  \item |fold|s, |scan|s and |reduce| operations can be computed in parallel using \alert{tree reduction}, reducing time from $O(n)$ to $O(\log n)$ (on infinite parallel hardware)
  \item ``\alert{map--reduce}'' is just the pattern of processing large amounts of data in an immutable collection by first \alert{map}ping the data (in parallel) into a monoid and then tree-\alert{reduc}ing the result (in parallel)
  \end{itemize}
\end{frame}


\subsection{Category Theory}

\begin{frame}[fragile]
  \frametitle{Category theory}
  \begin{itemize}
  \item A category $\mathcal{C}$ consists of a collection of \alert{objects}, $\operatorname{ob}(\mathcal{C})$, and \alert{morphisms}, $\operatorname{hom}(\mathcal{C})$. Each morphism is an ordered pair of objects (an arrow between objects). For $x,y\in \operatorname{ob}(\mathcal{C})$, the set of morphisms from $x$ to $y$ is denoted $\operatorname{hom}_{\mathcal{C}}(x,y)$. $f\in \operatorname{hom}_{\mathcal{C}}(x,y)$ is often written $f: x \longrightarrow y$.
  \item Categories are closed under \alert{composition}, so that if $f: x\longrightarrow y$ and $g: y\longrightarrow z$, then there must also exist a morphism $h: x\longrightarrow z$ written $h=g \circ f$.
  \item Composition is associative, so that $f\circ(g\circ h) = (f\circ g)\circ h$ for all composible $f, g, h\in \operatorname{hom}(\mathcal{C})$.
    \item For every $x\in \operatorname{ob}(\mathcal{C})$ there exists an \alert{identity} morphism $\operatorname{id}_x: x\longrightarrow x$, with the property that for any $f: x\longrightarrow y$ we have $f = f\circ \operatorname{id}_x = \operatorname{id}_y\circ f$.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Examples of categories}
  \begin{itemize}
  \item The category \textbf{Set} has an object for every \alert{set}, and its morphisms represent set \alert{functions}
    \begin{itemize}
    \item Note that this is a category, since functions are composable and we have identity functions, and function composition is associative
      \item Note that objects are ``atomic'' in category theory --- it is not possible to ``look inside'' the objects to see the set elements --- category theory is ``point-free''
    \end{itemize}
  \item For a pure FP language, we can form a category where objects represent \alert{types}, and morphisms represent \alert{functions} from one type to another
    \begin{itemize}
      \item In Haskell this category is often referred to as \textbf{Hask}
    \item This category is very similar to \textbf{Set}, in practice
    \item By modelling FP types and functions as a category, we can bring ideas and techniques from CT into FP
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{\textbf{Set} and \textbf{Hask}}
  \begin{itemize}
  \item $0\in\operatorname{ob}(\textbf{Set})$ is the empty set, $\emptyset$
    \begin{itemize}
    \item There is a unique morphism from $0$ to every other object --- it is an example of the concept of an \alert{initial object}
      \item $0$ in \textbf{Set} corresponds to the type |Void| in \textbf{Hask}, the type with no values
    \end{itemize}
  \item $1\in\operatorname{ob}(\textbf{Set})$ is a set containing exactly one element (and all such objects are \alert{isomorphic})
    \begin{itemize}
    \item There is a unique morphism from every other object to $1$ --- it is an example of the concept of a \alert{terminal object}
    \item $1$ in \textbf{Set} corresponds to the type |Unit| in \textbf{Hask}, the type with exactly one value, |()|
      \item Morphisms from $1$ to other objects must represent \alert{constant} functions, and hence must correspond to \alert{elements} of a set or \alert{values} of a type --- so we can use morphisms from $1$ to ``look inside'' our objects if we must...
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Monoid as a category with one object}
  \begin{itemize}
  \item Given our definition of a category, we can now reconsider the notion of a \alert{moniod} now as a \alert{category with one object}
  \item The object represents the ``type'' of the monoid, and the \alert{morphisms represent the ``values''}
  \item From our definition of a category, we know that there is an \alert{identity} morphism, that the morphisms are closed under \alert{composition}, and that they are \alert{associative}...
  \item For a monoid type object, $M$ in \textbf{Hask}, the morphisms represent \alert{functions}, $f_a: M\longrightarrow M$ defined by $f_a(m)=m\star a$
    \item Again, we see that it is the morphisms that really matter, and that these can be used to ``probe'' the ``internal structure'' of an object...
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Functors}
  \begin{itemize}
  \item A \alert{functor} is a mapping from one category to another which preserves some structure
  \item A functor $F$ from $\mathcal{C}$ to $\mathcal{D}$, written $F: \mathcal{C}\longrightarrow\mathcal{D}$ is a pair of functions (both denoted $F$):
    \begin{itemize}
    \item $F: \operatorname{ob}(\mathcal{C}) \longrightarrow \operatorname{ob}(\mathcal{D})$
    \item $F: \operatorname{hom}(\mathcal{C})\longrightarrow\operatorname{hom}(\mathcal{D})$, where $\forall f\in\operatorname{hom}(\mathcal{C})$, we have $F(f: x\longrightarrow y): F(x)\longrightarrow F(y)$
      \item In other words, if $f\in\operatorname{hom}_{\mathcal{C}}(x,y)$, then $F(f)\in\operatorname{hom}_{\mathcal{D}}(F(x),F(y))$
    \end{itemize}
  \item The functor must satisfy the \alert{functor laws}:
      \begin{itemize}
      \item $F(\operatorname{id}_x) = \operatorname{id}_{F(x)},\forall x\in\operatorname{ob}(\mathcal{C})$
      \item $F(f\circ g) = F(f)\circ F(g)$ for all composable $f,g\in \operatorname{hom}(\mathcal{C})$
      \end{itemize}
      %  \item The laws ensure that we can form a category, called \textbf{Cat}, which has categories as objects and functors as morphisms
      \item A functor $F:\mathcal{C}\longrightarrow\mathcal{C}$ is called an \alert{endofunctor} --- in the context of functional programming, the word functor usually refers to an endofunctor $F: \textbf{Hask}\longrightarrow\textbf{Hask}$
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Natural transformations}
  \begin{itemize}
  \item Often there are multiple functors between pairs of categories, and sometimes it is useful to be able to tranform one to another
    \item Suppose we have two functors $F,G: \mathcal{C}\longrightarrow\mathcal{D}$
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Monads}
  \begin{itemize}
  \item bar
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Applicative Functors}
  \begin{itemize}
  \item Lax monoidal functor (with a strength) ???
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{foo}
  \begin{itemize}
  \item bar
  \end{itemize}
\begin{lstlisting}[language=scala]
  val x = 1
  val y = f(x)
\end{lstlisting}
\end{frame}

\subsection{Scalable modelling and computation}

\begin{frame}[fragile]
  \frametitle{foo}
  \begin{itemize}
  \item bar
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{foo}
  \begin{itemize}
  \item bar
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{foo}
  \begin{itemize}
  \item bar
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{foo}
  \begin{itemize}
  \item bar
  \end{itemize}
\end{frame}



\begin{frame}[fragile]
  \frametitle{foo}
  \begin{itemize}
  \item bar
  \end{itemize}
\end{frame}


\subsection{Scalable statistical computing}

\section{Summary and conclusions}

\begin{frame}
  \frametitle{Summary}
  \begin{itemize}
  \item Obviously can't ``teach'' much about either FP or CT in a single talk/seminar
  \item Don't expect everyone to have understood everything
  \item The aim was to give a little insight into:
    \begin{itemize}
    \item Why FP is interesting, and inherently more modular, composable and scalable than imperative programming
    \item CT is a good model for composable computation, because it is a theory of structure and composition
      \item CT provides powerful abstractions which make FP easier, more modular, and more general
      \end{itemize}
  \end{itemize}
\end{frame}



\frame{
  \frametitle{Conclusions}
  \begin{itemize}
  \item We should approach the problem of statistical modelling and computation in a modular, composable, functional way, guided by underpinning principles from category theory
  \item To implement solutions to problems in statistical modelling and compution in a more scalable way, we need programming languages which are:
    \begin{itemize}
    \item \alert{Strongly statically typed} (but with type inference)
    \item \alert{Compiled} (but possibly to a VM)
    \item \alert{Functional} (with support for immutable values and higher-order functions)
      \item and have support for \alert{typeclasses} and \alert{higher-kinded types}, allowing the adoption of design patterns from category theory
    \end{itemize}
    \item \alert{Scala} and \alert{Spark} provide a nice illustration of the power of this approach, but there are other interesting languages, including: Haskell, (S)ML, OCaml, Frege, Eta, ...
  \end{itemize}
  }

\end{document}

